---
layout: blog 
title: "Week 7: Advanced models 1, part b"
---

## Character-based RNN with convolution
This week, we spent some more time working on that character RNN model after adding a convolution layer (with max pooling) on all the cells' hidden states. With the added convolution, the model now actually trains, and seems to perform about as well as the word RNN model. The model seems to overfit much more than before, so we experimented with adding dropout to the RNN.

**No dropout:**

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9315 | 0.8542 | 0.7980 | 0.8228 | 0.9251 | 0.6499 |

Confusion matrix:

```
[[19858   547]
 [ 1039  1716]]
```

**0.1 dropout:**

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9369 | 0.8676 | 0.8136 | 0.8376 | 0.9369 | 0.6791 |

Confusion matrix:

```
[[19902   503]
 [  959  1796]]
```

**0.2 dropout:**

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9372 | 0.8810 | 0.7977 | 0.8323 | 0.9384 | 0.6735 |

Confusion matrix:

```
[[20014   391]
 [ 1062  1693]]
```

We also tried reducing the size of the hidden state; results seem mixed; it seems likely that the reduced size performs just as well and that the difference is just normal variance, but it's a little hard to say.

**0.2 dropout, 50 hidden state size (down from 120)**

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9367 | 0.8892 | 0.7852 | 0.8265 | 0.9367 | 0.6664 |

Confusion matrix:

```
[[20079   326]
 [ 1139  1616]]
```

### Error analysis

Based on a few comments from the dev set, the character RNN does seem to do a better job at detecting slightly altered versions of profanity and such: for the comment

> ' JEFFREY MALL IS A NIG GER'

The char RNN output 0.9891 probability of attack, whereas the word RNN only had 0.2256. Incidentally, the actual proportion of human reviews who thought this was an attack was 0.3333, which brings up the first issue of borderline attacks. As expected, when even humans don't have consensus on whether a given comment is an attack, it's a little unreasonable to expect the model to do much better, but the model has a tendency to give borderline attacks a much higher attack probability than the proportion of human reviews.

(Note: for all following examples, the left number is the proportion of reviewers who voted for attack in the dataset, and the right is the probability of attack output from the model)

    0.444444 0.856087 '  Hahahahahaha you blocked me from Wikipedia because I hurt your feelings LOL'
    0.500000 0.997663 '  == WikiDicks ==  Nothing any of you old scholarly wannabe fucks can say about my article will negatively affect me more than this case has.  Have fun with your lame old selves.'

For many of these borderline cases, it definitely feels like the model's output is closer to the expected value than the reviewers', suggesting that perhaps we should be thresholding attacks vs. non-attacks from the proportion of reviewers a little lower, maybe at 30% instead of the current 50%.

Beyond that, there are many cases of the model being over-eager to classify short comments as attacks. For some of these, it seems that the model might be fabricating some context, or perhaps reading things sarcastically, since it's not too difficult to imagine some context where the comments could be interpreted as attacks. The high confidence it has that they are attacks is a little concerning, though.

    0.000000 0.966435 "  : You're such a wise woman  "
    0.000000 0.918345 '  oops ill give u the pass to mine if u get on yours'
    0.000000 0.982325 '  == Yo. 2 ==  *Currently hitting up*'
    0.000000 0.763677 '    ==DO NOT ERASE MY WORK UNTIL I AM FINISHED=='
    0.000000 0.587889 ' You missed one. We can play this game all night'

And then there are some examples of the model treating some ordinarily neutral words as bad since they are often used that way (e.g. race, wiki edits/revisions)
0.000000 0.649757 b' He looks very Jewish also if you can imagine him with a shaggy beard and some dark hair. '

Or failing to pick up that bad words are used neutrally.

    0.500000 0.985445 b"  == Guys I'm fucking bored give me something to do ==  Guys I'm fucking bored give me something to do"
    0.000000 0.906188 b'  ==Thats my name== You have blocked me but Kshitij is a Indian name. How can i change my name?? (  )'

Also, the model might be associating unknown characters with attacks (since only the most frequent 100 characters in the training set are recognized).
0.000000 0.661663 b'` :Looks like it was here. By the way, your Mandarin pinyin is a bit off, \xe6\xb0\x91 is m\xc3\xadn, not ming. \xc2\xa0 `'

On a bit of a side note, the model does some weird things with repeated characters or sequences of characters, but these numbers don't mean much on their own since, as mentioned already, the model doesn't work as well on short comments:

    1.0        "fuck"
    0.99999952 "fu ck"
    0.78159362 "fu         ck"
    0.99647254 "fu                   ck"

## Additional Word RNN Improvements

We also continued to work on improving the accuracy of our word RNN model, this time trying to specifically decrease the rate of false negatives. In particular, we wanted to see if we could try and more accurately classify attacks at the expense of sometimes misclassifying non-aggressive comments.

As before, we had only incremental improvement -- our best results are still our baseline model:

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9272 | 0.8236 | 0.8354 | 0.8294 | 0.9175 | 0.6589 |

Here is the corresponding confusion matrix:

| | Predicted OK | Predicted Attack |
| | ----------- | ---------- |
| **Actual OK** | 19503 | 902 |
| **Actual Attack** |  785 | 1970 |

We also attempted to merge all of the output states of our RNN instead of just looking at the last result, but that yielded in worse performance -- our model more or less became incapable of accurately categorizing attacks. This was not a terribly surprising result:

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9144 | 0.8133 | 0.7384 | 0.7687 | 0.8767 | 0.5465 |

Confusion matrix:

| | Predicted OK | Predicted Attack |
| | ----------- | ---------- |
| **Actual OK** | 19779 |  626 |
| **Actual Attack** | 1357 | 1398 |

## Aggression removal

We also experimented with the "remove aggression" filter we discussed last week. To recap, our idea was to try and rewrite existing comments to try and remove aggression. To do this, our idea was to:

Train an autoencoder on comments
Modify the loss function so it tries to simultaneously minimize the softmax sequence loss against the aggression or attack score (which is computed by one of our pre-trained model)
See what happens (??)

Currently, we're obtaining suboptimal results -- we decided to start by training just the auto-encoder as a sanity measure, but our model ultimately generates gibberish results -- a string of mostly repeated and gibberish text. Our loss is also relatively flat/not improving, which is unfortunate.

This may partially be because we're training on a small subset of our dataset for debugging purposes (1 epoch takes around 22 mins when using the full dataset), partially due to implementation errors, and partially because we haven't implemented attention yet.

## Feature inspection and error analysis

Out of curiosity, we also decided to try cracking open our bag-of-words model to analyze which unigrams and bigrams were most associated with positivity and negativity. The results were mostly predictable -- attacks focused around profanity. One somewhat interesting thing was that the most positive results tended to be focused around meta-discussion of wikipedia itself -- for example, see the bigram table below:

| Unigram | Probability OK | Probability Attack | | Bigram | Probability OK | Probability Attack |
| ------- | -------------- | -------------------| | ------ | -------------- | ------------------ |
| thank | 0.9952 | 0.0047 || redirect talk | 0.9965 |  0.0034 |     
| article | 0.9950 | 0.0049 || the article | 0.9963 |  0.0036 |
| thanks | 0.9950 | 0.0049 || this article | 0.9949 |  0.0050 |
| could | 0.9944 | 0.0055 || thanks for | 0.9939 |  0.0060 |
| please | 0.9943 | 0.0056 || that there | 0.9917 |  0.0082 |
| section | 0.9930 | 0.0069 || but not | 0.9912 |  0.0087 |
| agree | 0.9930 | 0.0069 || for the | 0.9910 |  0.0089 |
| welcome | 0.9923 | 0.0076 || article for | 0.9907 |  0.0092 |
| there | 0.9922 | 0.0077 || thank you | 0.9906 |  0.0093 |
| but | 0.9922 | 0.0077 || agree with | 0.9905 |  0.0094 |
| ...snip... | N/A | N/A || ...snip... | N/A | N/A |
| suck | 0.0203 | 0.9796 || bitch fuck | 0.0033 |  0.9966 |
| bitch | 0.0145 | 0.9854 || you fuck | 0.0031 |  0.9968 |
| you | 0.0121 | 0.9878 || fuck yourself | 0.0030 |  0.9969 |
| asshole | 0.0095 | 0.9904 || fucking asshole | 0.0019 |  0.9980 |
| ass | 0.0071 | 0.9928 || go fuck | 0.0011 |  0.9988 |
| shit | 0.0041 | 0.9958 || you fucking |  9.2084e-04 |   9.9907e-01 |
| stupid | 0.0011 | 0.9988 || fuck fuck |  6.7426e-04 |   9.9932e-01 |
| idiot |  7.4736e-04 |  9.9925e-01 || fuck off |  6.6430e-04 |   9.9933e-01 |
| fucking |  8.3943e-05 |  9.9991e-01 || the fuck |  5.1976e-04 |   9.9948e-01 |
| fuck |  1.9916e-05 |  9.9998e-01 || fuck you |  5.8870e-05 |   9.9994e-01 |

