---
layout: blog 
title: "Week 7: Advanced models 1, part b"
---

## Additional Word RNN Improvements

We also continued to work on improving the accuracy of our word RNN model, this time trying to specifically decrease the rate of false negatives. In particular, we wanted to see if we could try and more accurately classify attacks at the expense of sometimes misclassifying non-aggressive comments.

As before, we had only incremental improvement -- our best results are still our baseline model:

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9272 | 0.8236 | 0.8354 | 0.8294 | 0.9175 | 0.6589 |

Here is the corresponding confusion matrix:

| | Predicted OK | Predicted Attack |
| | ----------- | ---------- |
| Actual OK | 19503 | 902 |
| Actual Attack |  785 | 1970 |

We also attempted to merge all of the output states of our RNN instead of just looking at the last result, but that yielded in worse performance -- our model more or less became incapable of accurately categorizing attacks. This was not a terribly surprising result:

| Accuracy | Precision | Recall | F1 | ROC | Spearman |
| -------- | --------- | ------ | -- | --- | -------- |
| 0.9144 | 0.8133 | 0.7384 | 0.7687 | 0.8767 | 0.5465 |

Confusion matrix:

| | Predicted OK | Predicted Attack |
| | ----------- | ---------- |
| Actual OK | 19779 |  626 |
| Actual Attack | 1357 | 1398 |

## Aggression removal

We also experimented with the “remove aggression” filter we discussed last week. To recap, our idea was to try and rewrite existing comments to try and remove aggression. To do this, our idea was to:

Train an autoencoder on comments
Modify the loss function so it tries to simultaneously minimize the softmax sequence loss against the aggression or attack score (which is computed by one of our pre-trained model)
See what happens (??)

Currently, we’re obtaining suboptimal results -- we decided to start by training just the auto-encoder as a sanity measure, but our model ultimately generates gibberish results -- a string of mostly repeated and gibberish text. Our loss is also relatively flat/not improving, which is unfortunate.

This may partially be because we’re training on a small subset of our dataset for debugging purposes (1 epoch takes around 22 mins when using the full dataset), partially due to implementation errors, and partially because we haven’t implemented attention yet.

## Feature inspection and error analysis

Out of curiosity, we also decided to try cracking open our bag-of-words model to analyze which unigrams and bigrams were most associated with positivity and negativity. The results were mostly predictable -- attacks focused around profanity. One somewhat interesting thing was that the most positive results tended to be focused around meta-discussion of wikipedia itself -- for example, see the bigram table below:

| Unigram | Probability OK | Probability Attack | | Bigram | Probability OK | Probability Attack |
| ------- | -------------- | -------------------| | ------ | -------------- | ------------------ |
| thank | 0.9952 | 0.0047 || redirect talk | 0.9965 |  0.0034 |     
| article | 0.9950 | 0.0049 || the article | 0.9963 |  0.0036 |
| thanks | 0.9950 | 0.0049 || this article | 0.9949 |  0.0050 |
| could | 0.9944 | 0.0055 || thanks for | 0.9939 |  0.0060 |
| please | 0.9943 | 0.0056 || that there | 0.9917 |  0.0082 |
| section | 0.9930 | 0.0069 || but not | 0.9912 |  0.0087 |
| agree | 0.9930 | 0.0069 || for the | 0.9910 |  0.0089 |
| welcome | 0.9923 | 0.0076 || article for | 0.9907 |  0.0092 |
| there | 0.9922 | 0.0077 || thank you | 0.9906 |  0.0093 |
| but | 0.9922 | 0.0077 || agree with | 0.9905 |  0.0094 |
| ...snip... | N/A | N/A || ...snip... | N/A | N/A |
| suck | 0.0203 | 0.9796 || bitch fuck | 0.0033 |  0.9966 |
| bitch | 0.0145 | 0.9854 || you fuck | 0.0031 |  0.9968 |
| you | 0.0121 | 0.9878 || fuck yourself | 0.0030 |  0.9969 |
| asshole | 0.0095 | 0.9904 || fucking asshole | 0.0019 |  0.9980 |
| ass | 0.0071 | 0.9928 || go fuck | 0.0011 |  0.9988 |
| shit | 0.0041 | 0.9958 || you fucking |  9.2084e-04 |   9.9907e-01 |
| stupid | 0.0011 | 0.9988 || fuck fuck |  6.7426e-04 |   9.9932e-01 |
| idiot |  7.4736e-04 |  9.9925e-01 || fuck off |  6.6430e-04 |   9.9933e-01 |
| fucking |  8.3943e-05 |  9.9991e-01 || the fuck |  5.1976e-04 |   9.9948e-01 |
| fuck |  1.9916e-05 |  9.9998e-01 || fuck you |  5.8870e-05 |   9.9994e-01 |
