---
layout: blog 
title: "Week 4: Preliminary results"
---

## Overview

This week, we focused primarily on doing some preliminary investigations into
our datasets, and building several models.

## Overall methodology (and data characterization)

For this week, we focused on exploring Wikipedia's ["personal attacks" database][pad].
The database contains a collection of approximately 100,000 comments, each 
annotated by 5 workers indicating whether that comment contains a 
personal attack or not. This week, we labeled comments as an 'ATTACK' comment if
over 50% of the workers labeled it as containing an attack, and 'OK' otherwise,
and trained our model on this binary classification problem.

  [pad]: https://meta.wikimedia.org/wiki/Research:Detox/Data_Release#Wikipedia_Talk_Labels:_Personal_Attacks

We did some preliminary analysis on the dataset and discovered that...

- Approximately 85% of the comments were OK, approximately 15% were ATTACKs.
- The average number of words per comment is 87.3

During the next week, we plan on exploring our model against other aspects of the
dataset. In particular, the "personal attacks" dataset contains additional
information describing the nature of the attack. Wikipedia has also released
datasets measuring the level of aggression and toxicity, which we can explore.

## Bag-of-words model

We started by first implementing the basic model described within Wikipedia's
paper -- a bag-of-words model using tf-idf and logistic regression. We
obtained the following results:

| Metric    | Score     |
| --------- | --------- |
| Accuracy  | 0.940587  |
| Precision | 0.8927430 |
| Recall    | 0.569147  |
| F1        | 0.695035  |
| AUC       | 0.779942  |

We also obtained the following confusion matrix:

|            | OK    | ATTACK |
| ---------- | ----- | ------ |
| **OK**     | 20216 | 189    |
| **ATTACK** | 1187  | 1568   |


## Character n-gram model


The logistic regression character n-gram model takes counts of each n-gram of length 
1 through 5 that appears in the comments as features, and optimizes cross entropy loss 
using Adam, where positive (abusive) loss is given ten times more weight in an attempt 
to prevent the model from predicting all negatives. The model also uses l2 regularization 
with a lambda of 0.01. Unfortunately, the positive weighting on the loss doesnâ€™t seem to
be enough to prevent the model from eventually (after 12-14 epochs) predicting all negatives; 
we probably need to decrease the lambda, or increase the positive loss weighting further.

| Epoch | AUC | Precision | Recall | F1 | 
| ----- | --- | --------- | ------ | -- |
| 1  | 0.50083 | 0.11929 | 0.52341 | 0.19430 |
| 2  | 0.50132 | 0.11944 | 0.56333 | 0.19710 |
| 3  | 0.50266 | 0.11987 | 0.61125 | 0.20044 |
| 4  | 0.50515 | 0.12059 | 0.66823 | 0.20431 |
| 5  | 0.51059 | 0.12205 | 0.73611 | 0.20938 |
| 6  | 0.51536 | 0.12307 | 0.80907 | 0.21364 |
| 7  | 0.51829 | 0.12347 | 0.87985 | 0.21656 |
| 8  | 0.51552 | 0.12253 | 0.93720 | 0.21672 |
| 9  | 0.51113 | 0.12139 | 0.97640 | 0.21594 |
| 10 | 0.50420 | 0.11985 | 0.99165 | 0.21385 |
| 11 | 0.50184 | 0.11934 | 0.99745 | 0.21318 |
| 12 | 0.50069 | 0.11910 | 0.99927 | 0.21283 |
| 13 | 0.49999 | 0.11895 | 0.99963 | 0.21260 |
| 14 | 0.49989 | 0.11893 | 0.99963 | 0.21257 |
| 15 | 0.5     | 0.11895 | 1.0     | 0.21261 |
| 16 | 0.5     | 0.11895 | 1.0     | 0.21261 |
| 17 | 0.5     | 0.11895 | 1.0     | 0.21261 |

## LSTM + Bidirectional RNN model

We also experimented with a basic bidirectional RNN using LSTMs, softmax
cross-entropy loss, and the Adam optimizer within Tensorflow.

We primarily experimented with three parameters: the number of epochs, the size 
of the hidden state within each LSTM, the word length we fit each comment into. 
(If a comment has fewer words then the specified word length, we add padding, 
otherwise we truncate it).

We found that no matter how we tweaked the other parameters, the models 
converged after about 4 or 5 epochs.

**Varying epochs (Hidden state size = 120; Word cap of 100)**

| Epoch | Accuracy | Precision | Recall   | F1       | AUC      |
| ----- | -------- | --------- | -------- | -------- | -------- |
| 1     | 0.881046 | 0.0       | 0.0      | 0.0      | 0.0      |
| 2     | 0.882255 | 0.611111  | 0.027949 | 0.053454 | 0.512773 |
| 3     | 0.893696 | 0.575477  | 0.405445 | 0.475724 | 0.682531 |
| 4     | 0.928800 | 0.721200  | 0.654446 | 0.686204 | 0.810144 |
| 5     | 0.937349 | 0.778157  | 0.662069 | 0.715434 | 0.818292 |
| 6     | 0.935579 | 0.744483  | 0.698004 | 0.720495 | 0.832829 |
| 7     | 0.929103 | 0.693700  | 0.723412 | 0.708244 | 0.840142 |
| 8     | 0.925562 | 0.677087  | 0.715426 | 0.695729 | 0.834679 |

The word size cap ended up being the most relevant parameter. 
We probed on a few different sizes and obtained the following results:

**Varying length cap (Hidden state size = 120; results after 4 epochs)**

| Length cap | Accuracy | Precision | Recall   | F1       | AUC      |
| ---------- | -------- | --------- | -------- | -------- | -------- |
| 87         | 0.937695 | 0.783983  | 0.657350 | 0.715104 | 0.816447 |
| 100        | 0.928800 | 0.721200  | 0.654446 | 0.686204 | 0.810144 |
| 200        | 0.882168 | 0.842105  | 0.011615 | 0.022914 | 0.505660 |
| 500        | 0.881693 | 0.758621  | 0.007985 | 0.015805 | 0.503821 |

This result makes intuitive sense -- setting a large word cap would result
in potentially large amounts of padding, which would definitely have a 
negative effect on our model.

We did discover that tensorflow recently added support for dynamic-sized
RNNs, and the ability to have RNNS resize based on the current batch size,
so we plan on exploring this in the upcoming week to try and improve 
performance.

We did not fully have time to explore the effects of varying the hidden
state size.

**Varying hidden state size (Word length cap = 100, results after 4 epochs)**

| State size | Accuracy | Precision | Recall   | F1       | AUC      |
| ---------- | -------- | --------- | -------- | -------- | -------- |
| 5          | 0.917573 | 0.647079  | 0.675499 | 0.660984 | 0.812878 | 
| 80         | 0.939335 | 0.775061  | 0.690381 | 0.730275 | 0.831664 |
| 120        | 0.928800 | 0.721200  | 0.654446 | 0.686204 | 0.810144 |


## Upcoming work

For this upcoming week, we plan on...

- Further exploring our dataset
- Fixing our character n-gram model
- Refining our RNN model (e.g. exploring dropout, attention, dynamically-resizing RNNs, etc)
- Beginning on our stretch goals -- in particular, removing abuse and aggression.


