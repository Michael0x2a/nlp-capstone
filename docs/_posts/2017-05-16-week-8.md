---
layout: blog 
title: "Week 8: Advanced models 2"
---

## Altering attack classification threshold

Previously, we were classifying our data as attack/not-attack using a 0.5 split -- we
classified a comment as an attack if more than 50% of reviewers agreed it was an attack.

This week, we experimented with a 0.0 split -- that is, we classified a comment as an
attack if any *single* annotator considered it an attack. This has the net effect of
making our classifier possibly hyper-sensitive to possible attacks/abuse, but we
thought it was a worthwhile tradeoff, in part because it helps balance our dataset
(we now have 11551 comments classified as not-attack and 11609 classified as attack
in our train dataset), and in part because pragmatically a hyper-sensitive classifier
is likely potentially just as useful as one that isnâ€™t, given some human oversight.

We obtained the following results. As before, logistic regression and bag-of-words are
slightly out-performing our deep learning models. However, we still feel we have a
decent amount of work left to be done -- in particular, we plan on conducting more
in-depth error analysis of this adjusted dataset in the near future.

<table>
  <thead>
    <tr>
      <th>Classifier</th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>ROC</th>
      <th>Spearman</th>
      <th>Confusion Matrix</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Profanity filter (substring)</td>
      <td>0.5889</td><td>0.6696</td><td>0.3552</td>
      <td>0.4641</td><td>0.5895</td><td>0.2027</td>
      <td><code>[9517 2034]</code><br /><code>[7486 4123]</code></td>
    </tr>
    <tr>
      <td>Profanity filter (split on words)</td>
      <td>0.5567</td><td>0.9602</td><td>0.1207</td>
      <td>0.2144</td><td>0.5578</td><td>0.2380</td>
      <td><code>[11493 &nbsp;&nbsp;58]</code><br /><code>[10208  1401]</code></td>
    </tr>
    <tr>
      <td>Bag of words</td>
      <td>0.7541</td><td>0.7817</td><td>0.7070</td>
      <td>0.7424</td><td>0.8271</td><td>0.5108</td>
      <td><code>[9259 2292]</code><br /><code>[3402 8207]</code></td>
    </tr>
    <tr>
      <td>Logistic regression</td>
      <td>0.7522</td><td>0.7698</td><td>0.7215</td>
      <td>0.7449</td><td>0.8269</td><td>0.5056</td>
      <td><code>[9046 2505]</code><br /><code>[3233 8376]</code></td>
    </tr>
    <tr>
      <td>Word RNN</td>
      <td>0.6791</td><td>0.6759</td><td>0.6912</td>
      <td>0.6834</td><td>0.7276</td><td>0.3582</td>
      <td><code>[7703 3848]</code><br /><code>[3585 8024]</code></td>
    </tr>
    <tr>
      <td>Character RNN</td>
      <td>0.7071</td><td>0.7140</td><td>0.6934</td>
      <td>0.7036</td><td>0.7817</td><td>0.4145</td>
      <td><code>[8327 3224]</code><br /><code>[3559 8050]</code></td>
    </tr>
  </tbody>
</table>

## Stanford Politeness Corpus

We also experimented with other corpora -- in particular, the Stanford
Politeness Corpus. This corpus contains a collection of "requests" one
user made to another collected from Wikipedia and StackOverflow, along
with annotations recording the perceived politeness of the requests.

This is a comparatively smaller dataset compared to our Wikipedia corpus
-- it contains only a little over 10,000 entries overall, which is an
order of magnitude smaller then the Wikipedia corpus. We obtained our
train, dev, and test sets by shuffling the entries and splitting them
80%-20%-20%.

We then classified any entries with a normalized politeness score over
0.6 as being "polite"-- this produced an approximately 50-50 split.
We then obtained the following results:

<table>
  <thead>
    <tr>
      <th>Classifier</th>
      <th>Accuracy</th>
      <th>Precision</th>
      <th>Recall</th>
      <th>F1</th>
      <th>ROC</th>
      <th>Spearman</th>
      <th>Confusion Matrix</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Profanity filter (substring)</td>
      <td>0.4870</td><td>0.5489</td><td>0.0884</td>
      <td>0.1523</td><td>0.5047</td><td>0.0168</td>
      <td><code>[ 966   83]</code><br /><code>[1041  101]</code></td>
    </tr>
    <tr>
      <td>Profanity filter (split on words)</td>
      <td>0.4783</td><td>0.4000</td><td>0.0018</td>
      <td>0.0035</td><td>0.4994</td><td>-0.0116</td>
      <td><code>[1046  3]</code><br /><code>[1140  2]</code></td>
    </tr>
    <tr>
      <td>Bag of words</td>
      <td>0.6568</td><td>0.6533</td><td>0.7277</td>
      <td>0.6885</td><td>0.7174</td><td>0.3111</td>
      <td><code>[608 441]</code><br /><code>[311 831]</code></td>
    </tr>
    <tr>
      <td>Logistic regression</td>
      <td>0.6244</td><td>0.6360</td><td>0.6532</td>
      <td>0.6445</td><td>0.6780</td><td>0.2466</td>
      <td><code>[622 427]</code><br /><code>[396 746]</code></td>
    </tr>
    <tr>
      <td>Word RNN</td>
      <td>0.5860</td><td>0.5995</td><td>0.6200</td>
      <td>0.6096</td><td>0.6171</td><td>0.1694</td>
      <td><code>[576 473]</code><br /><code>[434 708]</code></td>
    </tr>
    <tr>
      <td>Character RNN</td>
      <td>0.6016</td><td>0.5959</td><td>0.7320</td>
      <td>0.6570</td><td>0.6483</td><td>0.1994</td>
      <td><code>[482 567]</code><br /><code>[306 836]</code></td>
    </tr>
  </tbody>
</table>
