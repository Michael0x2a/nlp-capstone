---
layout: blog 
title: "Week 5: Strawman and such"
---

## What we did this week

This week, we explored the following models:

- Using a straightforward profanity filter
- Logistic regression
    - Bag-of-words (1-2 ngrams)
    - Character n-grams (1-5 ngrams)
- Different RNN variations; mostly LSTM

In particular, for our RNN model, we tried experimenting with the following
parameters:

- Using a bidirectional RNN vs a simple one (the bidirectional one worked
  a bit better)
- Using a ``dynamic" bidirectional RNN, which could apparently use sequence
  lengths to disregard padding words (this made very little difference)
- Modifying dropout rate (also very little difference)
- Using different optimizers other then Adam (nope)
- Modifying the comment length cutoff (this made a difference, but I think we
  already came close to the optimum from last week)
- Using a character-based RNN, instead of a word-based one (this made things
  worse, although perhaps my hyperparameters were off)
- Tinkering with the learning rates of my optimizers (little difference, or made things worse)

We also tried doing some preliminary filtering of the data by writing code
to remove stop words and some stray fragments of wikimedia markup, but that
also appeared to make little difference.

We were told hyperparameter tuning was something of a black art, and after
my experiences last weekend, we can definitely attest that seems to be the
case.

Besides the tuning, we also wrote a webapp demo (which is currently offline
because the server I'm running it on costs like $1.50 an hour, or something).

## Results

Nevertheless, we did seem to obtain some reasonable results:

**Scores against personal attacks dataset**

| Classifier | Accuracy | Precision | Recall | F1 | ROC | Spearman | 
| ---------- | -------- | --------- | ------ | -- | --- | -------- |
| Profanity filter | 0.774870 | 0.623512 | 0.730009 | 0.637782 | 0.730009 | 0.337099 | 
| Bag-of-words | 0.940587 | 0.918485 | 0.779942 | 0.831062 | 0.955915 | 0.684549 |
| Char N-gram  | 0.846287 | 0.701206 | 0.846205 | 0.736786 | 0.923811 | 0.527858 |
| LSTM | 0.940717 | 0.891300 | 0.806389 | 0.841755 | 0.920935 | 0.692352 |
| GRU | 0.929231 | 0.833182 | 0.825303 | 0.829174 | 0.923901 | 0.658437 |

Our classifiers seemed to do reasonably well, in the grand scheme of things,
although the baseline Bag-of-Words model, somewhat disappointingly, seemed to
do best of all.

We suspect that our deep learning models will do better when we start exploring
things like aggression and toxicity, which hopefully require more context to
classify.

Here are some confusion matrices, which we included, because we can:

**Profanity filter**

| Not attack | Attack |
| ---------- | ------ |
| 16097 | 4308 |
| 906 | 1849 |

**Bag-of-words**

| Not attack | Attack | 
| ---------- | ------ |
| 20216 | 189 | 
| 1187 | 1568 |

**Character N-gram**

| Not attack | Attack | 
| ---------- | ------ |
| 14152 | 6253 | 
| 511 | 2244 |

**LSTM**

| Not attack | Attack | 
| ---------- | ------ |
| 19942 | 463 | 
| 903 | 1852 |

We found it interesting that the character n-grams skewed towards having 
so many false positives, while the other models skewed towards having 
more false negatives.

(The profanity filter, of course, had many false positives, but that doesn't
seem particularly surprising -- we do a naive substring match, so even
innoculous phrases like "hello" would trigger the predictor, since it
contains the substring "hell").

## Error analysis

We also did some preliminary error analysis. Our false negatives were 
generally pretty bad, and we found it somewhat difficult to understand
why they were being misclassified (though that may change as we conduct more
thorough analysis later this week).

Our false positives were a little more interesting.

- In some cases, the original sentence just had a borderline label. For 
  example, our classifiers thought that "Maybe you should quit being a douche 
  bag, and then people will listen to you." was pretty aggressive, but this
  comment actually had a borderline rating -- only 40% of the annotators
  thought this contained a personal attack.
- In other cases, our classifiers are abnormally biased towards certain 
  keywords (such as "Iran", "vandalism", anything to do with edits). For 
  example:

    - The Kurdish People is a Irania People,  The Kurdish language is a 
      Irania language.
    - == Wow ==  You are a reverting machine!

- And of course, we had our fair share of bizarre false positives. For example,
  our classifiers apparently thought that "Wow! You are the best, mate! Thanks 
  a ton!!" contained a personal attack -- perhaps it took the repeated 
  exclamation points as a sign of insanity?

## Things to explore

Apart from our stretch goals, which haven't really changed from last week,
we have a collection of other things we can try exploring.

- We have some datasets we haven't touched (the internet argument corpus,
  the Stanford politeness corpus).
- It may be interesting to try integrating insights from sentiment analysis,
  which we haven't really looked at yet. 
- Maybe meet with a TA to see if we actually understand how to use Tensorflow.
- Conduct a more thorough error analysis.
- Examining how our classifiers adapt to different communities, which may have
  different standards and norms then the Wikipedia community.

We also discovered another potential issue that we may want to add to our
set of stretch goals -- handling memes.

For example, during our demo, somebody apparently submitted the following text:

> What the fuck did you just fucking say about me, you little bitch? 
> I'll have you know I graduated top of my class in the Navy Seals, and 
> I've been involved in numerous secret raids on Al-Quaeda, and I have over 
> 300 confirmed kills. I am trained in gorilla warfare and I'm the top 
> sniper in the entire US armed forces. You are nothing to me but just 
> another target. I will wipe you the fuck out with precision the likes 
> of which has never been seen before on this Earth, mark my fucking words. 
> You think you can get away with saying that shit to me over the Internet? 
> Think again, fucker. As we speak I am contacting my secret network of spies 
> across the USA and your IP is being traced right now so you better prepare 
> for the storm, maggot. The storm that wipes out the pathetic little thing 
> you call your life. You're fucking dead, kid. I can be anywhere, anytime, 
> and I can kill you in over seven hundred ways, and that's just with my 
> bare hands. Not only am I extensively trained in unarmed combat, but I 
> have access to the entire arsenal of the United States Marine Corps and 
> I will use it to its full extent to wipe your miserable ass off the face 
> of the continent, you little shit. If only you could have known what unholy 
> retribution your little "clever" comment was about to bring down upon you, 
> maybe you would have held your fucking tongue. But you couldn't, you didn't, 
> and now you're paying the price, you goddamn idiot. I will shit fury all 
> over you and you will drown in it. You're fucking dead, kiddo

This is, of course, the Navy Seal copypasta.

Our classifiers naturally uniformally thought this contained a high degree 
of personal attacks, aggression, and toxicity. 

However, pragmatically, this comment is meant as a joke and would most likely
_not_ be considered to actually be an attack/toxic on many internet 
communities. (It would probably be counted as spam on Wikipedia, however).

In an ideal world, our classifier should somehow be able to pick up on cases
like these and avoid flagging them (since they're false positives), but it
seems this would require us somehow acquiring and using real-world context
during classification, which seems challenging.

It's unclear how we would handle cases like these.
