---
layout: blog 
title: "Week 3: Formal proposal"
---

## Project Objectives

Our overarching goal is to explore and analyze the quality of conversations 
in online communities. More specifically, we plan on building tools to detect 
user comments that are either abusive (contain slurs, personal attacks, etc) 
or toxic (negatively contribute to the tone of the conversation).

Our stretch goals currently focus on building tools that can preempt abuse 
and toxicity by either detecting problematic topics or posts before they 
occur, or by suggesting alternative wordings/problematic phrases to users 
before they post to try and encourage more positive discourse.

## Minimum Viable Plan

For our minimum viable plan, we plan on building two tools:

- A tool that can determine whether or not a given comment includes abuse (in 
  particular, personal attacks)
- A tool that can determine the level of toxicity of a comment

To do this, we plan on primarily using a 
[dataset released by the wikimedia foundation][0] which contains human-labeled 
datasets judging the level of toxicity and aggression of a comment, and 
whether or not it contains a personal attack. These three datasets contains 
100k comments each.

We also found several other datasets with varying types of annotations
(listed later) that we plan on using to further test and evaluate our models.

We plan on starting by reproducing baseline results using character n-grams as 
inputs to a logistic regression model and a multilayer perceptron. We will 
then create an LSTM network and compare its performance to these two baseline 
approaches.

The majority, or all, of our approaches will be implemented using TensorFlow.

  [0]: https://figshare.com/projects/Wikipedia_Talk/16731


## Stretch Goals

We have a variety of stretch goals and possible avenues for research.

Our plan is to implement the majority of these additional goals after 
completing our MVP.

- **Modify the wikipedia dataset to include thread context.**

  Currently, the wikipedia dataset is constructed such that each user comment 
  is presented in isolation. We hypothesize this makes classifying 
  certain comments difficult -- if a comment is categorized as toxic, is it 
  because they were quoting something non-toxic? Because they were replying 
  in kind? Or perhaps seemingly non-toxic text is actually sarcastic in 
  context and is consequently very toxic.

  The collective wikipedia dataset contains enough information to let us 
  approximately reconstruct the context per each individual topic or page. 
  While this is not really an NLP or deep learning challenge, it may be a 
  necessary preliminary step before we can explore our other stretch goals.


- **Can we predict what kinds of topics/parent comments are likely to invite 
  abuse?**

  [Wulczyn, et. al.][wiki-paper] observed that personal attacks in particular 
  tend to be clustered closely together (perhaps one attack invites another?). 
  Purely from a community management standpoint, it may be more cost-effective 
  to detect controversial topics/signs of toxicity and preempt them instead 
  of responding after the fact.

  We plan on augmenting our models with existing work on sentiment 
  analysis and topic detection when exploring this approach.
  
- **Can we predict which users are likely to post abusive/toxic comments?**

  Similarly, rather than monitoring topics, can we predict which users are 
  more likely to post abusive comments? If we make the assumption that 
  certain users are effectively permanently toxic (and are likely to try and 
  evade bans), it may be more cost-effective to try and monitor problematic 
  users rather than comments.

- **Automatically rewording abusive/toxic posts to preserve content but 
  removing toxicity**

  To further try and preempt toxicity, it may be worth trying to detect toxic 
  posts before they're posted, and presenting the poster with either a list 
  of phrases they wrote that are likely to be perceived negatively, or by 
  proposing alternative (and more positive) wording they can use instead.

  We tentatively plan on treating this as a machine translation problem, 
  where the input language is "toxic" text, and the output is non-toxic text, 
  and implementing an encoder-decoder RNN.

  [wiki-paper]: https://arxiv.org/pdf/1610.08914.pdf

## Available Resources and Literature Survey

### Papers

- [Ex Machina: personal attacks seen at scale](https://arxiv.org/pdf/1610.08914.pdf)

- [Abusive Language Detection in Online User Content](http://www2016.net/proceedings/proceedings/p145.pdf)

- [Measuring the Reliability of Hate Speech Annotations: The Case of the 
  European Refugee Crisis](https://arxiv.org/pdf/1701.08118.pdf)

- [Automatic identification of personal insults on social news sites](https://pdfs.semanticscholar.org/3fa4/d63e0194cdbd909c579456830e0a7c909242.pdf)


### Datasets:

- [Wikipedia Talk dataset](https://figshare.com/projects/Wikipedia_Talk/16731)

- [The Internet Argument Corpus](https://nlds.soe.ucsc.edu/iac2)

- [Hate Speech Twitter Annotations](https://github.com/ZeerakW/hatespeech)

- [Kaggle: Detecting Insults in Social Commentary](https://www.kaggle.com/c/detecting-insults-in-social-commentary/data)


