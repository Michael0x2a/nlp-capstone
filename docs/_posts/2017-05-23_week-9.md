---
layout: blog 
title: "Week 9: Endgame, part 1"
---

## Stanford dataset

Last week, we mentioned that we looked at using the Stanford Politeness Corpus since the Wikipedia corpus’s attacks mainly consist of comments with profanity, that can pretty easily be detected using just n-grams. This week, we continued looking into improving results by training using mean squared error instead of cross-entropy error. The evaluation was still done using the same metrics as before, with the 0.6 being the splitting point between being considered polite and impolite.
 
Word RNN with soft labels

| Accuracy | Precision | Recall | F1     | ROC    | Spearman |
| -------- | --------- | ------ | ------ | ------ | -------- |
| 0.6098   | 0.5959    | 0.7811 | 0.6760 | 0.6421 | 0.2194   |

```
[[444 605]
 [250 892]]
```
 
These results are pretty similar to before, and are in fact slightly better, but the concerning factor is something that can’t be seen in just these numbers: practically all of the outputs are right around the mean label value of the dataset. 
 
Word RNN with soft labels on different corpora

| Dataset | Label average | Label standard deviation | Output average | Output standard deviation | Spearman correlation |
| ------- | ------------- | ------------------------ | -------------- | ------------------------- | -------------------- |
| Stanford | 0.5941 | 0.1108 | 0.5686 | 0.0047 | 0.0698 |
| Wikipedia | 0.1720 | 0.2671 | 0.1465 | 0.2246 | 0.6322 |
 
To reiterate, we scaled the expected labels from the dataset to be between 0 and 1 and chose 0.6 as the split since that splits the data fairly evenly, i.e. it’s close to the mean value of the scaled soft labels. In our outputs, the mean was about the same as in the expected labels, but the standard deviation was much lower; basically, the best the model could do was predict right at about the mean, with very little certainty about its results. Additionally, the Spearman rank correlation coefficient is pretty abysmal for the Stanford corpus. On the other hand, results are much closer to what we’d expect for the Wikipedia corpus, suggesting that using the Stanford Politeness Corpus will not be an easy task.
 
## Twitter hate speech dataset

In part because the stanford dataset was performing so poorly, we decided to look into finding a potential replacement dataset. We found a recently published [dataset of tweets](https://github.com/t-davidson/hate-speech-and-offensive-language) where each tweet was annotated as being either offensive, hateful, or neither. For the purposes of our evaluation, we grouped together offensive and hateful tweets into one single category (as opposed to treating them as two distinct labels), and obtained the following results:
 
| Classifier | Accuracy | Precision | Recall | F1     | ROC    | Spearman | Confusion Matrix |
| ---------  | -------- | --------- | ------ | ------ | ------ | -------- | ---------------- |
| Profanity filter | 0.8291 | 0.9730 | 0.8182 | 0.8889 | 0.8514 | 0.5699 | `[ 721   94]`<br />`[ 753 3389]` |
| Bag of words | 0.9227 | 0.9312 | 0.9800 | 0.9549 | 0.9714 | 0.6963 | `[ 515  300]`<br />`[  83 4059]` |
| Logistic regression | 0.9437 | 0.9658 | 0.9669 | 0.9663 | 0.9777 | 0.7947 | `[ 673  142]`<br />`[ 137 4005]` |
| Word RNN | 0.9254 | 0.9480    | 0.9635 | 0.9557 | 0.9380 | 0.7199   | `[ 596  219]`<br />`[ 151 3991]` |
| Char RNN | 0.8606   | 0.9101    | 0.9244 | 0.9172 | 0.8868 | 0.4765   | `[ 437  378]`<br />`[ 313 3829]` |

 
 
Note: for our word RNN, we capped the comment size to 32 words; for our character RNN we first experimented with 140 characters (the former maximum length of a tweet) and then to 60 characters.
 
Our results were more or less the same as our Wikipedia results, albeit by only relatively small margins.

## LSTM attention for comment rewriting

In light of the difficulty of predicting the Stanford corpus, the lack of additional results from our twitter corpus, and our limited time remaining in the quarter, we decided to switch focus (attention, if you will) to comment rewriting instead. The idea is to train the word RNN using 
