---
layout: blog 
title: "Week 6: Advanced models, attempt 1"
---

## Refactoring and technical debt

This week, we finished up refactoring the interfaces for our models, with the end goal of making it easier for us to compare models and tune hyperparameters and to pay off lingering technical debt. In particular, we:

- Refactored our models so they followed the same interfaces
- Added a program to let us easily swap out datasets, models, and hyperparameters from the command line
- Added static types (via mypy) to our codebase

While we were at it, we also configured our TensorFlow models to persistently save summaries, allowing us to compare training progress between runs on the same model.

## Enhancements and fixes to LSTM model

### Better UNK handling

We realized we had a bug in our UNK handling that, in essence, caused our code to reject words at random, biased slightly towards low frequency words, instead of targeting specifically low frequency words.

After doing some error analysis, we realized that perhaps improving our word tokenization process would improve the quality of our models. We started filtering out and normalizing some artifacts of Wikipedia’s comment formatting syntax (in particular, related to headers and quotes) and added some code to replace URLs with a special “UNK-URL” symbol.

These changes combined led to some minor incremental improvements in our results:

| Classifier | Accuracy | Precision | Recall | F1 | ROC | Spearman |
| ---------- | -------- | --------- | ------ | -- | --- | -------- |
| Old LSTM model | 0.9407 | 0.8913 | 0.8064 | 0.8418 | 0.9209 | 0.6924 |
| New LSTM model | 0.9562 | 0.9031 | 0.8261 | 0.8573 | 0.9420 | 0.6876 |

We suspect we may need to further re-tune our hyperparemeters based on these changes.


### Character LSTM

We tried playing around with the hyperparameters of the character LSTM a bit more, hoping to replicate the improvement in performance between the word and character n-grams, but still haven’t been able to train it to do much better than random, nevermind the word LSTM.

![Training loss][loss]

This is what the training loss looks like on two runs; the top run had an extra 10 times weight on positive loss applied, since the model predicted mostly not-attack on the first (bottom) run. As you can probably see, the loss isn’t improving at all over time; assuming the model isn’t horribly broken somehow, some of the hyperparameters must still be pretty far from optimal. Since most of the values were copied from the word LSTM, but the input and vocabulary sizes have already been adjusted to more reasonable values for characters, perhaps the size of the LSTM cells’ states need to be increased more, or perhaps we need to be using trainable character embedding vectors rather than one-hot vectors.

[loss]: http://i.imgur.com/kWAoEow.png

## Experiment: removing aggression from posts

This week, we also began progress on one of our experimental goals; automatically removing aggression from English sentences. Our current plan is to start by taking a seq2seq autoencoder, then modifying the loss function to try to simultaneously minimize the difference between the input sentence and the generated output sentence as well as the aggression score of the generated sentence. 

Unfortunately, we weren’t really able to get this working by this blog post. We spent the bulk of our time trying to get a grasp on how seq2seq works and working through other implementation details, though we do hope to make some progress in the next few days, perhaps after visiting office hours.

Once we do get this working, we expect that automatically evaluating the effectiveness of our aggression/toxicity remover will likely be difficult. Although we hypothesize that the modified loss function described above would cause our model to behave approximately in the manner we want, we would need to assess the grammaticality and semantic similarity of our output sentence to the input in order to measure the suitability of the output, which is likely to be difficult.

